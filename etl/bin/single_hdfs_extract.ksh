#! /bin/ksh
# Script generated by software licensed from Ab Initio.
# Use and disclosure are subject to Ab Initio confidentiality and license terms.
export AB_HOME;AB_HOME=${AB_HOME:-/legato/softlib/abinitio/abinitio-V3-1-4}
export MPOWERHOME;MPOWERHOME="$AB_HOME"
export AB_COMPONENTS;AB_COMPONENTS="$AB_HOME"'/Projects/root/components'
export PATH
typeset _ab_uname=`uname`
case "$_ab_uname" in
Windows_* )
    PATH="$AB_HOME/bin;$PATH" ;;
CYGWIN_* )
    PATH="`cygpath "$AB_HOME"`/bin:/usr/local/bin:/usr/bin:/bin:$PATH" ;;
* )
    PATH="$AB_HOME/bin:$PATH" ;;
esac
unset ENV
export AB_REPORT;AB_REPORT=${AB_REPORT:-'monitor=300 processes scroll=true'}
unset GDE_EXECUTION

export AB_COMPATIBILITY;AB_COMPATIBILITY=3.1.4.4

# Deployed execution script for graph "single_hdfs_extract", compiled at Friday, October 04, 2013 11:14:19 using GDE version 3.0.3.1
export AB_JOB;AB_JOB=${AB_JOB_PREFIX:-""}single_hdfs_extract
# Begin Ab Initio shell utility functions

: ${_ab_uname:=$(uname)}

function __AB_INVOKE_PROJECT
{
  typeset _AB_PROJECT_KSH="$1" ; shift
  typeset _AB_PROJECT_DIR="$1" ; shift
  typeset _AB_DEFINE_OR_EXECUTE="$1" ; shift
  typeset _AB_START_OR_END="$1" ; shift
  # Check that the project exists:
  if [ ! -r "$_AB_PROJECT_KSH" ] ; then
    print -r -u2 Warning: Cannot find common sandbox script: "$_AB_PROJECT_KSH"
    if [ ! -z "${_AB_CALLING_PROJECT:=}" ] ; then
      print -r -u2 Please check the common sandbox settings for the calling project: "$_AB_CALLING_PROJECT"
    fi
  fi
  if [ $# -gt 0 ] ; then
    . "$_AB_PROJECT_KSH" "$_AB_PROJECT_DIR" "$_AB_DEFINE_OR_EXECUTE" "$_AB_START_OR_END"  "$@"
  else
    . "$_AB_PROJECT_KSH" "$_AB_PROJECT_DIR" "$_AB_DEFINE_OR_EXECUTE" "$_AB_START_OR_END" 
  fi;
}

function __AB_DOTIT
{
  if [ $# -gt 0 ] ; then
    .  "$@"
  fi
}

function __AB_QUOTEIT {
  typeset queue q qq qed lotsaqs s trail
  q="'"
  qq='"'
  if [ X"$1" = X"" ] ; then
    print $q$q
    return
  fi
  lotsaqs=${q}${qq}${q}${qq}${q}
  if [ ${#1} -ge 10000 ]; then
    print -r -- "$1" | sed "s/$q/$lotsaqs/g; 1s/^/$q/; \$s/\$/$q/"
  else
    queue=${1%$q}
    if [ X"$queue" != X"$1" ] ; then
      trail="${qq}${q}${qq}" 
    else 
      trail=""
    fi
    oldIFS="$IFS"
    IFS=$q
    set -- $queue
    IFS="$oldIFS"
    print -rn "$q$1"
    shift
    for s; do
      print -rn "$lotsaqs$s"
    done
    print -r $q$trail
  fi
}

function __AB_dirname {
    case $_ab_uname in
    Windows_* | CYGWIN_* )
        typeset d='' p="$1"
        # Strip drive letter colon, if present, and put it into d.
        case $p in
        [A-Za-z]:* )
            d=${p%%:*}:
            p=${p#??}
            ;;
        esac
        # Remove trailing separators, though not the last character in the
        # pathname.
        while : true; do
            case $p in
            ?*[/\\] )
                p=${p%[/\\]} ;;
            * )
                break ;;
            esac
        done
        if [[ "$p" = ?*[/\\]* ]] ; then
            print -r -- "$d${p%[/\\]*}"
        elif [[ "$p" = [/\\]* ]] ; then
            print "$d/"
        else
            print "$d." 
        fi
        ;;
    * ) # Unix
        typeset p="$1"
        # Remove trailing separators, though not the last character in the
        # pathname.
        while : true; do
            case $p in
            ?*/ )
                p="${p%/}" ;;
            * )
                break ;;
            esac
        done
        case $p in
        ?*/* )
            print -r -- "${p%/*}" ;;
        /* )
            print / ;;
        * )
            print . ;;
        esac
        ;;
    esac
}

function __AB_concat_pathname {
    case $_ab_uname in
    Windows_* | CYGWIN_* )
        # Does not handle all cases of concatenating partially absolute
        # pathnames, those with only one of a drive letter or an initial
        # separator.
        case $2 in
        [/\\]* | [A-Za-z]:* )
            print -r -- "$2"
            ;;
        * )
            case $1 in
            # Assume that empty string means ".".  Avoid adding a
            # redundant separator.
            '' | *[/\\] )
                print -r -- "$1$2" ;;
            * )
                print -r -- "$1/$2" ;;
            esac
            ;;
        esac
        ;;
    * ) # Unix
        case $2 in
        /* )
            print -r -- "$2"
            ;;
        * )
            case $1 in
            # Assume that empty string means ".".  Avoid adding a
            # redundant separator.
            '' | */ )
                print -r -- "$1$2" ;;
            * )
                print -r -- "$1/$2" ;;
            esac
            ;;
        esac
        ;;
    esac
}

function __AB_COND {
if [ X"$1" = X0  -o X"$1" = Xfalse -o X"$1" = XFalse -o X"$1" = XF -o X"$1" = Xf ] ; then
  print "0"
else
  print "1"
fi
}

# End Ab Initio shell utility functions
export AB_GRAPH_NAME;AB_GRAPH_NAME=single_hdfs_extract

# Host Setup Commands:
. /dw/etl/mstr_cfg/etlenv.setup
_AB_PROXY_DIR=single_hdfs_extract-ProxyDir-$$
rm -rf "${_AB_PROXY_DIR}"
mkdir "${_AB_PROXY_DIR}"
print -r -- "" > "${_AB_PROXY_DIR}"'/GDE-Parameters'
function __AB_CLEANUP_PROXY_FILES
{
   rm -rf "${_AB_PROXY_DIR}"
   rm -rf "${AB_EXTERNAL_PROXY_DIR}"
   return
}
trap '__AB_CLEANUP_PROXY_FILES' EXIT
# Work around pdksh bug: the EXIT handler is not executed upon a signal.
trap '_AB_status=$?; __AB_CLEANUP_PROXY_FILES; exit $_AB_status' HUP INT QUIT TERM
if [ $# -gt 0 -a X"$1" = X"-help" ]; then
print -r -- 'Usage: single_hdfs_extract.ksh <ETL_ID> <FILE_ID> <JOB_CONFIG> <SOURCE_NAME_NOT_USED> <DATA_FILENAME_TMP> [-DML_FILENAME <DML_FILENAME>] -UOW_FROM <UOW_FROM> -UOW_TO <UOW_TO>'
exit 1
fi

# Command Line Processing
function _AB_PARSE_ARGUMENTS {
   unset ETL_ID
   unset FILE_ID
   unset JOB_CONFIG
   unset SOURCE_NAME_NOT_USED
   unset DATA_FILENAME_TMP
   unset DML_FILENAME
   unset UOW_FROM
   unset UOW_TO
   _ab_index_var=0
   if [ $# -gt 0 ]; then
      export ETL_ID;      ETL_ID="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export FILE_ID;      FILE_ID="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export JOB_CONFIG;      JOB_CONFIG="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export SOURCE_NAME_NOT_USED;      SOURCE_NAME_NOT_USED="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export DATA_FILENAME_TMP;      DATA_FILENAME_TMP="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   while [ $# -gt 0 ]; do
   _ab_kwd="${1}"
   let _ab_index_var=_ab_index_var+1
   shift
   case ${_ab_kwd} in
   # DML_FILENAME - optional dml file name - used to override the default dml file
     -DML_FILENAME )
      export DML_FILENAME;      DML_FILENAME="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
   # UOW_FROM - Unit of Work
     -UOW_FROM )
      export UOW_FROM;      UOW_FROM="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
   # UOW_TO - Next Unit of Work
     -UOW_TO )
      export UOW_TO;      UOW_TO="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
   # CNDTL_EXTRACT_COMPRESS_LEVEL - fILE COMPRESSION LEVEL (0-9)
     -CNDTL_EXTRACT_COMPRESS_LEVEL )
      CNDTL_EXTRACT_COMPRESS_LEVEL="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -CNDTL_EXTRACT_COMPRESS )
      CNDTL_EXTRACT_COMPRESS="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -OUTPUT_DML )
      export OUTPUT_DML;      OUTPUT_DML="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -REFORMAT_TRANS_FILE )
      export REFORMAT_TRANS_FILE;      REFORMAT_TRANS_FILE="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -XFORM_REJECT_FILE )
      XFORM_REJECT_FILE="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -XFORM_ERROR_FILE )
      XFORM_ERROR_FILE="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
   * )
      if [ X"${_AB_USED_ARGUMENTS[_ab_index_var]}" != X1 ]; then
         print -r -- 'Unexpected command line argument found: '"${_ab_kwd}"
         print -r -- 'Usage: single_hdfs_extract.ksh <ETL_ID> <FILE_ID> <JOB_CONFIG> <SOURCE_NAME_NOT_USED> <DATA_FILENAME_TMP> [-DML_FILENAME <DML_FILENAME>] -UOW_FROM <UOW_FROM> -UOW_TO <UOW_TO>'
         exit 1
      fi
   esac
   done
}
if [ $# -gt 0 ]; then
   _AB_PARSE_ARGUMENTS "$@"
else
   _AB_PARSE_ARGUMENTS
fi

if [ X"${ETL_ID:-}" = X"" ]; then
   print -r -- 'Required parameter ETL_ID undefined'
   print -r -- 'Usage: single_hdfs_extract.ksh <ETL_ID> <FILE_ID> <JOB_CONFIG> <SOURCE_NAME_NOT_USED> <DATA_FILENAME_TMP> [-DML_FILENAME <DML_FILENAME>] -UOW_FROM <UOW_FROM> -UOW_TO <UOW_TO>'
   exit 1
fi
export FILE_ID;FILE_ID=${FILE_ID:-1}

if [ X"${JOB_CONFIG:-}" = X"" ]; then
   print -r -- 'Required parameter JOB_CONFIG undefined'
   print -r -- 'Usage: single_hdfs_extract.ksh <ETL_ID> <FILE_ID> <JOB_CONFIG> <SOURCE_NAME_NOT_USED> <DATA_FILENAME_TMP> [-DML_FILENAME <DML_FILENAME>] -UOW_FROM <UOW_FROM> -UOW_TO <UOW_TO>'
   exit 1
fi

if [ X"${SOURCE_NAME_NOT_USED:-}" = X"" ]; then
   print -r -- 'Required parameter SOURCE_NAME_NOT_USED undefined'
   print -r -- 'Usage: single_hdfs_extract.ksh <ETL_ID> <FILE_ID> <JOB_CONFIG> <SOURCE_NAME_NOT_USED> <DATA_FILENAME_TMP> [-DML_FILENAME <DML_FILENAME>] -UOW_FROM <UOW_FROM> -UOW_TO <UOW_TO>'
   exit 1
fi
export DATA_FILENAME_TMP;DATA_FILENAME_TMP=${DATA_FILENAME_TMP:-"$ETL_ID"'.'"$FILE_ID"'.dat'}
export DML_FILENAME;DML_FILENAME=${DML_FILENAME:-"$ETL_ID"'.read.dml'}
export UOW_FROM;UOW_FROM=${UOW_FROM:-""}
export UOW_TO;UOW_TO=${UOW_TO:-""}
export INPUT_FILE;INPUT_FILE=$(eval print $SOURCE_NAME_NOT_USED)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter INPUT_FILE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export JOB_ENV;JOB_ENV=extract
export UOW_DATE;UOW_DATE='$(print '"$UOW_TO"' | cut -c1-8)'
export ETL_CFG_FILE;ETL_CFG_FILE="$DW_CFG"'/'"$ETL_ID"'.cfg'
export SUBJECT_AREA;SUBJECT_AREA=${ETL_ID%%.*}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter SUBJECT_AREA of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export TABLE_ID;TABLE_ID=${ETL_ID##*.}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter TABLE_ID of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export AB_JOB;AB_JOB=$(if [ $ETL_ENV ]
then
   print $AB_JOB.$ETL_ID.$FILE_ID.$ETL_ENV.$JOB_ENV
else
   print $AB_JOB.$ETL_ID.$FILE_ID.$JOB_ENV
fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter AB_JOB of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export DW_SA_ARC;DW_SA_ARC="$DW_ARC"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export DW_SA_DAT;DW_SA_DAT="$DW_DAT"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export DW_SA_LOG;DW_SA_LOG=${DW_SA_LOG:-$DW_LOG/$JOB_ENV/$SUBJECT_AREA}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter DW_SA_LOG of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export DW_SA_TMP;DW_SA_TMP="$DW_TMP"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export FILE_DATETIME;FILE_DATETIME=${CURR_DATETIME:-$(date '+%Y%m%d-%H%M%S')}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter FILE_DATETIME of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export BATCH_SEQ_NUM;BATCH_SEQ_NUM=$(( $(<$DW_SA_DAT/$TABLE_ID.extract.batch_seq_num.dat) + 1))
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter BATCH_SEQ_NUM of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
CNDTL_EXTRACT_COMPRESS_LEVEL=$(grep "^CNDTL_EXTRACT_COMPRESS_LEVEL\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-""})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_EXTRACT_COMPRESS_LEVEL of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
CNDTL_EXTRACT_COMPRESS=$(grep "^CNDTL_EXTRACT_COMPRESS\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT;  print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_EXTRACT_COMPRESS of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export DATA_FILENAME;DATA_FILENAME=$(if [[ -n $UOW_TO ]]
  then
     if [[ $DATA_FILENAME_TMP = "N" ]]
     then
        print $(grep "^DATA_FILENAME\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print $VALUE)
     else
        eval print $DATA_FILENAME_TMP
     fi
  else
     if [[ $DATA_FILENAME_TMP = "N" ]]
     then
        print $(grep "^DATA_FILENAME\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print $VALUE.$BATCH_SEQ_NUM)
     else
        eval print $DATA_FILENAME_TMP.$BATCH_SEQ_NUM
     fi
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter DATA_FILENAME of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export IN_DIR;IN_DIR=${IN_DIR:-$(grep "^IN_DIR\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print $VALUE/$JOB_ENV/$SUBJECT_AREA)}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter IN_DIR of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export WRK_DIR;WRK_DIR=$(grep "^WRK_DIR\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print $VALUE/$SUBJECT_AREA)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter WRK_DIR of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export LAST_EXTRACT_TYPE;LAST_EXTRACT_TYPE=$(grep "^LAST_EXTRACT_TYPE\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print $VALUE)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter LAST_EXTRACT_TYPE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export CNDTL_LAST_EXTRACT_ROLLUP;CNDTL_LAST_EXTRACT_ROLLUP=$(if [[ $LAST_EXTRACT_TYPE = "R" ]]; then print 1; else print 0; fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_LAST_EXTRACT_ROLLUP of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export CNDTL_LAST_EXTRACT_VARIABLE;CNDTL_LAST_EXTRACT_VARIABLE=$(if [[ $LAST_EXTRACT_TYPE = "V" ]]; then print 1; else print 0; fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_LAST_EXTRACT_VARIABLE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export LAST_EXTRACT_VALUE_FILE;LAST_EXTRACT_VALUE_FILE=$(if [[ $LAST_EXTRACT_TYPE != @("N"|"U") ]]
  then
     print $DW_SA_DAT/$TABLE_ID.$FILE_ID.last_extract_value.dat
  else
     print a
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter LAST_EXTRACT_VALUE_FILE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export UOW_FROM_REFORMAT_CODE;UOW_FROM_REFORMAT_CODE=$(if [[ $LAST_EXTRACT_TYPE == "U" ]]
  then
   grep "^UOW_FROM_REFORMAT_CODE\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print ${VALUE:-0}
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter UOW_FROM_REFORMAT_CODE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export UOW_TO_REFORMAT_CODE;UOW_TO_REFORMAT_CODE=$(if [[ $LAST_EXTRACT_TYPE == "U" ]]
  then
   grep "^UOW_TO_REFORMAT_CODE\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print ${VALUE:-0}
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter UOW_TO_REFORMAT_CODE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export FROM_EXTRACT_VALUE;FROM_EXTRACT_VALUE=$(if [[ $LAST_EXTRACT_TYPE == @("V"|"R") ]]
  then
     print $(<$LAST_EXTRACT_VALUE_FILE)
  elif [[ $LAST_EXTRACT_TYPE == "U" ]]
  then
     print $(eval $DW_MASTER_EXE/dw_infra.reformat_timestamp.ksh $UOW_FROM $UOW_FROM_REFORMAT_CODE)
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter FROM_EXTRACT_VALUE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export TO_EXTRACT_VALUE_FUNCTION;TO_EXTRACT_VALUE_FUNCTION=$(if [[ $LAST_EXTRACT_TYPE = "V" ]]
  then
     print $(grep "^TO_EXTRACT_VALUE_FUNCTION\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print $VALUE)
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter TO_EXTRACT_VALUE_FUNCTION of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export TO_EXTRACT_VALUE;TO_EXTRACT_VALUE=$(if [[ $LAST_EXTRACT_TYPE == "V" ]]
  then
     print $($TO_EXTRACT_VALUE_FUNCTION)
  elif [[ $LAST_EXTRACT_TYPE == "U" ]]
  then
     print $(eval $DW_MASTER_EXE/dw_infra.reformat_timestamp.ksh $UOW_TO $UOW_TO_REFORMAT_CODE)
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter TO_EXTRACT_VALUE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export INPUT_DML;INPUT_DML="$DW_DML"'/'"$DML_FILENAME"
export UOW_APPEND;UOW_APPEND=$(if [[ $LAST_EXTRACT_TYPE == "U" ]]
  then
    print ".$UOW_TO"
  else
    print ""
  fi
 )
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter UOW_APPEND of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export OUTPUT_FILE;OUTPUT_FILE=$(if [ $CNDTL_EXTRACT_COMPRESS != 1 ]
  then
    eval print $IN_DIR/$DATA_FILENAME
  else
    eval print $IN_DIR/$DATA_FILENAME.gz
  fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter OUTPUT_FILE of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export ROLLUP_FIELD;ROLLUP_FIELD=$(grep "^ROLLUP_FIELD\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print $VALUE)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter ROLLUP_FIELD of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export RECORD_COUNT_FILE;RECORD_COUNT_FILE="$DW_SA_TMP"'/'"$TABLE_ID"'.ex.'"$FILE_ID"'.record_count.dat'
export CNDTL_EXTRACT_REFORMAT;CNDTL_EXTRACT_REFORMAT=$(grep "^CNDTL_EXTRACT_REFORMAT\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_EXTRACT_REFORMAT of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export CNDTL_EXTRACT_PARTITION;CNDTL_EXTRACT_PARTITION=$(grep "^CNDTL_EXTRACT_PARTITION\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_EXTRACT_PARTITION of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export CNDTL_EXTRACT_PARTITION_LAYOUT;CNDTL_EXTRACT_PARTITION_LAYOUT=$(if [[ $CNDTL_EXTRACT_PARTITION -eq 1 ]]
  then
        grep "^CNDTL_EXTRACT_PARTITION_LAYOUT\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print ${VALUE:-$OUTPUT_FILE}
  else
        print $OUTPUT_FILE
  fi
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_EXTRACT_PARTITION_LAYOUT of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export OUTPUT_DML;OUTPUT_DML=$(if (($CNDTL_EXTRACT_REFORMAT))
then
  print $DW_DML/$ETL_ID.extract_write.dml
else
  print $INPUT_DML
fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter OUTPUT_DML of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
export REFORMAT_TRANS_FILE;REFORMAT_TRANS_FILE="$DW_XFR"'/'"$ETL_ID"'.extract_reformat.xfr'
export PERF_PARALLEL_READ;PERF_PARALLEL_READ=$(grep "^HADOOP_PARALLEL_READ\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter PERF_PARALLEL_READ of single_hdfs_extract', interpretation 'shell'
   exit $mpjret
fi
XFORM_REJECT_FILE="$DW_SA_TMP"'/'"$TABLE_ID"'.hdfs.ex.reformat.rej'
XFORM_ERROR_FILE="$DW_SA_TMP"'/'"$TABLE_ID"'.hdfs.ex.reformat.err'
(
   # Parameters of Deflate
   condition=$CNDTL_EXTRACT_COMPRESS
   mpjret=$?
   if [ 0 -ne $mpjret ] ; then
      print -- Error evaluating: 'parameter condition of Deflate', interpretation 'shell'
      exit $mpjret
   fi
   print -rn Deflate__condition= >>${_AB_PROXY_DIR}/GDE-Parameters
   __AB_QUOTEIT "${condition}" >> ${_AB_PROXY_DIR}/GDE-Parameters
   conditionInputPort=in
   conditionOutputPort=out
   condition_interpretation='Replace with flow'
   compression=$CNDTL_EXTRACT_COMPRESS_LEVEL
   mpjret=$?
   if [ 0 -ne $mpjret ] ; then
      print -- Error evaluating: 'parameter compression of Deflate', interpretation 'shell'
      exit $mpjret
   fi
   print -rn Deflate__compression= >>${_AB_PROXY_DIR}/GDE-Parameters
   __AB_QUOTEIT "${compression}" >> ${_AB_PROXY_DIR}/GDE-Parameters
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then exit $mpjret ; fi
(
   # Parameters of Hadoop Input Dataset
   input_files="$INPUT_FILE"
   job_config="$DW_DBC"'/'"$JOB_CONFIG"
   HADOOP_HOME="$HADOOP_HOME"
   Layout=""
   _AB_FILE_NAME__out_metadata="$INPUT_DML"
   if [ -r "${_AB_FILE_NAME__out_metadata}" ]; then
      out_metadata=$(< "${_AB_FILE_NAME__out_metadata}")
      mpjret=$?
      if [ 0 -ne $mpjret ] ; then
         print -- Error evaluating: 'parameter out_metadata of Hadoop_Input_Dataset', interpretation 'shell'
         exit $mpjret
      fi
   else
      print -r -- 'Warning: cannot read '"'""${_AB_FILE_NAME__out_metadata}""'"' to define parameter out_metadata of Hadoop_Input_Dataset'
   fi
   condition=$( if [[ $PERF_PARALLEL_READ = 1 ]]; then print "0"; else print "1"; fi)
   mpjret=$?
   if [ 0 -ne $mpjret ] ; then
      print -- Error evaluating: 'parameter condition of Hadoop_Input_Dataset', interpretation 'shell'
      exit $mpjret
   fi
   print -rn Hadoop_Input_Dataset__condition= >>${_AB_PROXY_DIR}/GDE-Parameters
   __AB_QUOTEIT "${condition}" >> ${_AB_PROXY_DIR}/GDE-Parameters
)
mpjret=$?
if [ 0 -ne $mpjret ] ; then exit $mpjret ; fi
. ./${_AB_PROXY_DIR}/GDE-Parameters

#+Script Start+  ==================== Edits in this section are preserved.
m_env -v


if [ -z $LAST_EXTRACT_TYPE ]
then
   print "$0: Error: LAST_EXTRACT_TYPE variable not set"
   exit 4
fi

if [[ $LAST_EXTRACT_TYPE != "N" && -z $FROM_EXTRACT_VALUE ]]
then
   print "$0: Error: FROM_EXTRACT_VALUE variable not set"
   exit 4
fi

if [[ $LAST_EXTRACT_TYPE = "V" && -z $TO_EXTRACT_VALUE ]]
then
   print "$0: Error: TO_EXTRACT_VALUE variable not set"
   exit 4
fi

if [[ $LAST_EXTRACT_TYPE = "R" && -z $ROLLUP_FIELD ]]
then
   print "$0: Error: ROLLUP_FIELD variable not set"
   exit 4
fi

if [ -z $BATCH_SEQ_NUM ]
then
   print "$0: Error: BATCH_SEQ_NUM variable not set"
   exit 4
fi




#+End Script Start+  ====================
# Check that the "mp" program is found correctly on the PATH
case "$_ab_uname" in
  Windows_* )
    _ab_expected_mp=$AB_HOME/bin/mp.exe ;;
  * )
    _ab_expected_mp=$AB_HOME/bin/mp
esac
if [ ! -x "$_ab_expected_mp" ]; then
  print "\n*** ERROR: executable $_ab_expected_mp not found"
  exit 1
fi
_ab_found_mp=$(whence mp)
if [ "$_ab_found_mp" = "" ] || [ "$_ab_found_mp" -ot "$_ab_expected_mp" ] || [ "$_ab_found_mp" -nt "$_ab_expected_mp" ]; then
  if [ "$_ab_found_mp" = "" ]; then
    print "\n*** ERROR: mp not found on PATH"
  else
    case "$_ab_uname" in
      CYGWIN_* )
        _ab_found_mp=`cygpath -m "$_ab_found_mp"` ;;
    esac
    print "\n*** ERROR: Wrong mp found on the PATH: $_ab_found_mp"
    print "           Should be via \$AB_HOME/bin: $_ab_expected_mp"
  fi
  print "\nCheck Setup Script in Host Connections Settings and Script Start in Graph Settings for PATH modifications"
  print "Active PATH=$PATH"
  exit 1
fi
if [ -f "$AB_HOME/bin/ab_catalog_functions.ksh" ]; then . ab_catalog_functions.ksh; fi
mv "${_AB_PROXY_DIR}" "${AB_JOB}"'-single_hdfs_extract-ProxyDir'
_AB_PROXY_DIR="${AB_JOB}"'-single_hdfs_extract-ProxyDir'
print -r -- 'include "~ab_home/Projects/root/components/Internet/TCP/tcpheader.dml";

type key_value_pair = record
  void(big endian integer(4)) key;
  void(big endian integer(4)) value;
end;

metadata type = record
  tcpheader header;
  key_value_pair request_body;
end;' > "${_AB_PROXY_DIR}"'/Hadoop_Parallel_Read-3.dml'
print -r -- 'out :: reformat(in) =
begin
  out.value :: reinterpret_as(string(big endian integer(4)),in.request_body.value);
end;' > "${_AB_PROXY_DIR}"'/Reformat-4.xfr'
print -r -- 'record
  string("\n") value;
end;' > "${_AB_PROXY_DIR}"'/Reformat-5.dml'
print -r -- 'record big endian real(8) sum_of_crc; big endian real(8) sum_of_lengths; unsigned big endian integer(4) xor_of_crc; unsigned big endian integer(4) record_count; end' > "${_AB_PROXY_DIR}"'/Compute_Checksum_of_Input_Flow-6.dml'
print -r -- 'record decimal("\n") record_count; end' > "${_AB_PROXY_DIR}"'/Record_Count_File-7.dml'
print -r -- 'string('"'"'\n'"'"')' > "${_AB_PROXY_DIR}"'/Conditional_Extract_Reformat-10.dml'
print -r -- 'out::rollup(in) =
begin
  out.last_extract_value :: (string("\n")) first_defined(max(in.'"${ROLLUP_FIELD}"'), NULL);                  
end;' > "${_AB_PROXY_DIR}"'/Get_Max_Value_for_Extract_Field-11.xfr'
print -r -- 'record
  string("\n") last_extract_value;
end;' > "${_AB_PROXY_DIR}"'/Last_Extract_Value_File_1-12.dml'
print -r -- 'record
  string(1) a;
end;' > "${_AB_PROXY_DIR}"'/Send_Primer_Row_to_Reformat-13.dml'
print -r -- 'out::reformat(in) =
begin
  out.last_extract_value :: $TO_EXTRACT_VALUE;
end;' > "${_AB_PROXY_DIR}"'/Update_Last_Extract_Value_File-14.xfr'

mp job ${AB_JOB}

# Layouts:
mp layout layout1 "$CNDTL_EXTRACT_PARTITION_LAYOUT"
mp layout layout2 file:.
mp layout layout3 'file:'"$LAST_EXTRACT_VALUE_FILE"

# Record Formats (Metadata):
mp metadata metadata1 -file "$INPUT_DML"
mp metadata metadata2 -file "${_AB_PROXY_DIR}"'/Hadoop_Parallel_Read-3.dml'
mp metadata metadata3 -file "${_AB_PROXY_DIR}"'/Reformat-5.dml'
mp metadata metadata4 -file "${_AB_PROXY_DIR}"'/Compute_Checksum_of_Input_Flow-6.dml'
mp metadata metadata5 -file "${_AB_PROXY_DIR}"'/Record_Count_File-7.dml'
mp metadata metadata6 -file "$OUTPUT_DML"
mp metadata metadata7 -file "${_AB_PROXY_DIR}"'/Conditional_Extract_Reformat-10.dml'
mp metadata metadata8 -file "${_AB_PROXY_DIR}"'/Last_Extract_Value_File_1-12.dml'
mp metadata metadata9 -file "${_AB_PROXY_DIR}"'/Send_Primer_Row_to_Reformat-13.dml'

export AB_CATALOG;AB_CATALOG=${AB_CATALOG:-"${XX_CATALOG}"}
# Catalog Usage: Creating temporary catalog using lookup files only
m_rmcatalog -catalog GDE-single_hdfs_extract-${AB_JOB}.cat > /dev/null 2>&1
m_mkcatalog -catalog GDE-single_hdfs_extract-${AB_JOB}.cat
SAVED_CATALOG="${AB_CATALOG}"
export AB_CATALOG;AB_CATALOG='GDE-single_hdfs_extract-'"${AB_JOB}"'.cat'
# 
# Initialize condition variables to user-specified conditions
# 
AB_USERCOND_single_hdfs_extract=1
AB_IS_LIVE_single_hdfs_extract=1
AB_USERCOND_Hadoop_Input_Dataset="$Hadoop_Input_Dataset__condition"
AB_USERCOND_Hadoop_Input_Dataset=$(__AB_COND "${AB_USERCOND_Hadoop_Input_Dataset}")
AB_IS_LIVE_Hadoop_Input_Dataset=1
AB_HAS_DATA_Flow_24=1
AB_USERCOND_Parallel_Read_Hadoop="$PERF_PARALLEL_READ"
AB_USERCOND_Parallel_Read_Hadoop=$(__AB_COND "${AB_USERCOND_Parallel_Read_Hadoop}")
AB_IS_LIVE_Parallel_Read_Hadoop=1
AB_HAS_DATA_Flow_1=1
AB_USERCOND_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_=1
AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_=1
AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1=1
AB_USERCOND_Parallel_Read_Hadoop_Reformat=1
AB_IS_LIVE_Parallel_Read_Hadoop_Reformat=1
AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2=1
AB_USERCOND_Parallel_Read_Hadoop_Redefine_Format=1
AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format=1
AB_USERCOND_Replicate="$CNDTL_LAST_EXTRACT_ROLLUP"
AB_USERCOND_Replicate=$(__AB_COND "${AB_USERCOND_Replicate}")
AB_IS_LIVE_Replicate=1
AB_HAS_DATA_Flow_16=1
AB_HAS_DATA_Flow_5=1
AB_USERCOND_Conditional_Partition_by_Round_robin="$CNDTL_EXTRACT_PARTITION"
AB_USERCOND_Conditional_Partition_by_Round_robin=$(__AB_COND "${AB_USERCOND_Conditional_Partition_by_Round_robin}")
AB_IS_LIVE_Conditional_Partition_by_Round_robin=1
AB_HAS_DATA_Flow_11=1
AB_USERCOND_Retain_Flow_to_Flow="$CNDTL_EXTRACT_PARTITION"
AB_USERCOND_Retain_Flow_to_Flow=$(__AB_COND "${AB_USERCOND_Retain_Flow_to_Flow}")
AB_IS_LIVE_Retain_Flow_to_Flow=1
AB_HAS_DATA_Flow_23=1
AB_HAS_DATA_Flow_7=1
AB_USERCOND_Compute_Checksum_of_Input_Flow=1
AB_IS_LIVE_Compute_Checksum_of_Input_Flow=1
AB_HAS_DATA_Flow_8=1
AB_USERCOND_Capture_out_record_count_from_Checksum=1
AB_IS_LIVE_Capture_out_record_count_from_Checksum=1
AB_HAS_DATA_Flow_9=1
AB_USERCOND_Record_Count_File=1
AB_IS_LIVE_Record_Count_File=1
AB_USERCOND_Conditional_Extract_Reformat="$CNDTL_EXTRACT_REFORMAT"
AB_USERCOND_Conditional_Extract_Reformat=$(__AB_COND "${AB_USERCOND_Conditional_Extract_Reformat}")
AB_IS_LIVE_Conditional_Extract_Reformat=1
AB_HAS_DATA_Flow_21=1
AB_HAS_DATA_Flow_10=1
AB_HAS_DATA_Flow_2=1
AB_USERCOND_Deflate="$Deflate__condition"
AB_USERCOND_Deflate=$(__AB_COND "${AB_USERCOND_Deflate}")
AB_IS_LIVE_Deflate=1
AB_HAS_DATA_Flow_22=1
AB_USERCOND_Output_File=1
AB_IS_LIVE_Output_File=1
AB_USERCOND_XForm_Error_File="$CNDTL_EXTRACT_REFORMAT"
AB_USERCOND_XForm_Error_File=$(__AB_COND "${AB_USERCOND_XForm_Error_File}")
AB_IS_LIVE_XForm_Error_File=1
AB_USERCOND_XForm_Reject_File="$CNDTL_EXTRACT_REFORMAT"
AB_USERCOND_XForm_Reject_File=$(__AB_COND "${AB_USERCOND_XForm_Reject_File}")
AB_IS_LIVE_XForm_Reject_File=1
AB_USERCOND_Send_From_Extract_Value_to_Rollup="$CNDTL_LAST_EXTRACT_ROLLUP"
AB_USERCOND_Send_From_Extract_Value_to_Rollup=$(__AB_COND "${AB_USERCOND_Send_From_Extract_Value_to_Rollup}")
AB_IS_LIVE_Send_From_Extract_Value_to_Rollup=1
AB_HAS_DATA_Flow_3=1
AB_USERCOND_Get_Max_Value_for_Extract_Field="$CNDTL_LAST_EXTRACT_ROLLUP"
AB_USERCOND_Get_Max_Value_for_Extract_Field=$(__AB_COND "${AB_USERCOND_Get_Max_Value_for_Extract_Field}")
AB_IS_LIVE_Get_Max_Value_for_Extract_Field=1
AB_HAS_DATA_Flow_6=1
AB_USERCOND_Last_Extract_Value_File_1="$CNDTL_LAST_EXTRACT_ROLLUP"
AB_USERCOND_Last_Extract_Value_File_1=$(__AB_COND "${AB_USERCOND_Last_Extract_Value_File_1}")
AB_IS_LIVE_Last_Extract_Value_File_1=1
AB_USERCOND_Send_Primer_Row_to_Reformat="$CNDTL_LAST_EXTRACT_VARIABLE"
AB_USERCOND_Send_Primer_Row_to_Reformat=$(__AB_COND "${AB_USERCOND_Send_Primer_Row_to_Reformat}")
AB_IS_LIVE_Send_Primer_Row_to_Reformat=1
AB_HAS_DATA_Flow_4=1
AB_USERCOND_Update_Last_Extract_Value_File="$CNDTL_LAST_EXTRACT_VARIABLE"
AB_USERCOND_Update_Last_Extract_Value_File=$(__AB_COND "${AB_USERCOND_Update_Last_Extract_Value_File}")
AB_IS_LIVE_Update_Last_Extract_Value_File=1
AB_HAS_DATA_Flow_12=1
AB_USERCOND_Last_Extract_Value_File="$CNDTL_LAST_EXTRACT_VARIABLE"
AB_USERCOND_Last_Extract_Value_File=$(__AB_COND "${AB_USERCOND_Last_Extract_Value_File}")
AB_IS_LIVE_Last_Extract_Value_File=1
# 
# Compute condition variables by considering the conditions of neighboring components
# 
done=false
while [ $done = false ] ; do
   done=true
   Temp="${AB_IS_LIVE_Hadoop_Input_Dataset}"
   let AB_IS_LIVE_Hadoop_Input_Dataset="(AB_HAS_DATA_Flow_24) && (AB_USERCOND_Hadoop_Input_Dataset)"
   if [ X"${AB_IS_LIVE_Hadoop_Input_Dataset}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_24}"
   let AB_HAS_DATA_Flow_24="(AB_IS_LIVE_Hadoop_Input_Dataset) && ((AB_IS_LIVE_Replicate) || ((AB_HAS_DATA_Flow_16) || (AB_HAS_DATA_Flow_5)))"
   if [ X"${AB_HAS_DATA_Flow_24}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Parallel_Read_Hadoop}"
   let AB_IS_LIVE_Parallel_Read_Hadoop="AB_USERCOND_Parallel_Read_Hadoop"
   if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_1}"
   let AB_HAS_DATA_Flow_1="((AB_IS_LIVE_Parallel_Read_Hadoop) && ((AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format) || (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2))) && ((AB_IS_LIVE_Replicate) || ((AB_HAS_DATA_Flow_16) || (AB_HAS_DATA_Flow_5)))"
   if [ X"${AB_HAS_DATA_Flow_1}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_}"
   let AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_="(AB_IS_LIVE_Parallel_Read_Hadoop) && (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1)"
   if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1}"
   let AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1="(AB_IS_LIVE_Parallel_Read_Hadoop) && ((AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_) && ((AB_IS_LIVE_Parallel_Read_Hadoop_Reformat) || (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2)))"
   if [ X"${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Parallel_Read_Hadoop_Reformat}"
   let AB_IS_LIVE_Parallel_Read_Hadoop_Reformat="(AB_IS_LIVE_Parallel_Read_Hadoop) && ((AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1) && (((AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2) != 0)))"
   if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Reformat}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2}"
   let AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2="(AB_IS_LIVE_Parallel_Read_Hadoop) && (((AB_IS_LIVE_Parallel_Read_Hadoop_Reformat) || (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1)) && ((AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format) || (AB_HAS_DATA_Flow_1)))"
   if [ X"${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format}"
   let AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format="(AB_IS_LIVE_Parallel_Read_Hadoop) && ((AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2) && (AB_HAS_DATA_Flow_1))"
   if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Replicate}"
   let AB_IS_LIVE_Replicate="((AB_HAS_DATA_Flow_1) || (AB_HAS_DATA_Flow_24)) && ((AB_USERCOND_Replicate) || ((((AB_HAS_DATA_Flow_1+AB_HAS_DATA_Flow_24) > 1)) || (((AB_HAS_DATA_Flow_16+AB_HAS_DATA_Flow_5) > 1))))"
   if [ X"${AB_IS_LIVE_Replicate}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_16}"
   let AB_HAS_DATA_Flow_16="((AB_IS_LIVE_Replicate) || ((AB_HAS_DATA_Flow_1) || (AB_HAS_DATA_Flow_24))) && ((AB_IS_LIVE_Conditional_Partition_by_Round_robin) || (AB_HAS_DATA_Flow_11))"
   if [ X"${AB_HAS_DATA_Flow_16}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_5}"
   let AB_HAS_DATA_Flow_5="((AB_IS_LIVE_Replicate) || ((AB_HAS_DATA_Flow_1) || (AB_HAS_DATA_Flow_24))) && (AB_IS_LIVE_Get_Max_Value_for_Extract_Field)"
   if [ X"${AB_HAS_DATA_Flow_5}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Conditional_Partition_by_Round_robin}"
   let AB_IS_LIVE_Conditional_Partition_by_Round_robin="((AB_HAS_DATA_Flow_16) && (AB_HAS_DATA_Flow_11)) && ((AB_USERCOND_Conditional_Partition_by_Round_robin) || ((((AB_HAS_DATA_Flow_16) > 1)) || (((AB_HAS_DATA_Flow_11) > 1))))"
   if [ X"${AB_IS_LIVE_Conditional_Partition_by_Round_robin}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_11}"
   let AB_HAS_DATA_Flow_11="((AB_IS_LIVE_Conditional_Partition_by_Round_robin) || (AB_HAS_DATA_Flow_16)) && ((AB_IS_LIVE_Retain_Flow_to_Flow) || ((AB_HAS_DATA_Flow_23) || (AB_HAS_DATA_Flow_7)))"
   if [ X"${AB_HAS_DATA_Flow_11}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Retain_Flow_to_Flow}"
   let AB_IS_LIVE_Retain_Flow_to_Flow="(AB_HAS_DATA_Flow_11) && ((AB_USERCOND_Retain_Flow_to_Flow) || ((((AB_HAS_DATA_Flow_11) > 1)) || (((AB_HAS_DATA_Flow_23+AB_HAS_DATA_Flow_7) > 1))))"
   if [ X"${AB_IS_LIVE_Retain_Flow_to_Flow}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_23}"
   let AB_HAS_DATA_Flow_23="((AB_IS_LIVE_Retain_Flow_to_Flow) || (AB_HAS_DATA_Flow_11)) && ((AB_IS_LIVE_Conditional_Extract_Reformat) || (AB_HAS_DATA_Flow_21))"
   if [ X"${AB_HAS_DATA_Flow_23}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_7}"
   let AB_HAS_DATA_Flow_7="((AB_IS_LIVE_Retain_Flow_to_Flow) || (AB_HAS_DATA_Flow_11)) && ((AB_IS_LIVE_Compute_Checksum_of_Input_Flow) || (AB_HAS_DATA_Flow_8))"
   if [ X"${AB_HAS_DATA_Flow_7}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Compute_Checksum_of_Input_Flow}"
   let AB_IS_LIVE_Compute_Checksum_of_Input_Flow="(AB_HAS_DATA_Flow_7) && (AB_HAS_DATA_Flow_8)"
   if [ X"${AB_IS_LIVE_Compute_Checksum_of_Input_Flow}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_8}"
   let AB_HAS_DATA_Flow_8="((AB_IS_LIVE_Compute_Checksum_of_Input_Flow) || (AB_HAS_DATA_Flow_7)) && ((AB_IS_LIVE_Capture_out_record_count_from_Checksum) || (AB_HAS_DATA_Flow_9))"
   if [ X"${AB_HAS_DATA_Flow_8}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Capture_out_record_count_from_Checksum}"
   let AB_IS_LIVE_Capture_out_record_count_from_Checksum="(AB_HAS_DATA_Flow_8) && (((AB_HAS_DATA_Flow_9) != 0))"
   if [ X"${AB_IS_LIVE_Capture_out_record_count_from_Checksum}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_9}"
   let AB_HAS_DATA_Flow_9="(AB_IS_LIVE_Capture_out_record_count_from_Checksum) || (AB_HAS_DATA_Flow_8)"
   if [ X"${AB_HAS_DATA_Flow_9}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Conditional_Extract_Reformat}"
   let AB_IS_LIVE_Conditional_Extract_Reformat="((AB_HAS_DATA_Flow_23) && (((AB_HAS_DATA_Flow_21) != 0))) && ((AB_USERCOND_Conditional_Extract_Reformat) || ((((AB_HAS_DATA_Flow_23) > 1)) || (((AB_HAS_DATA_Flow_21) > 1))))"
   if [ X"${AB_IS_LIVE_Conditional_Extract_Reformat}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_21}"
   let AB_HAS_DATA_Flow_21="((AB_IS_LIVE_Conditional_Extract_Reformat) || (AB_HAS_DATA_Flow_23)) && ((AB_IS_LIVE_Deflate) || (AB_HAS_DATA_Flow_22))"
   if [ X"${AB_HAS_DATA_Flow_21}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_10}"
   let AB_HAS_DATA_Flow_10="((AB_IS_LIVE_Conditional_Extract_Reformat) && (AB_IS_LIVE_XForm_Reject_File)) && (AB_HAS_DATA_Flow_21)"
   if [ X"${AB_HAS_DATA_Flow_10}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_2}"
   let AB_HAS_DATA_Flow_2="((AB_IS_LIVE_Conditional_Extract_Reformat) && (AB_IS_LIVE_XForm_Error_File)) && (AB_HAS_DATA_Flow_21)"
   if [ X"${AB_HAS_DATA_Flow_2}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Deflate}"
   let AB_IS_LIVE_Deflate="((AB_HAS_DATA_Flow_21) && (AB_HAS_DATA_Flow_22)) && ((AB_USERCOND_Deflate) || ((((AB_HAS_DATA_Flow_21) > 1)) || (((AB_HAS_DATA_Flow_22) > 1))))"
   if [ X"${AB_IS_LIVE_Deflate}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_22}"
   let AB_HAS_DATA_Flow_22="(AB_IS_LIVE_Deflate) || (AB_HAS_DATA_Flow_21)"
   if [ X"${AB_HAS_DATA_Flow_22}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_XForm_Error_File}"
   let AB_IS_LIVE_XForm_Error_File="(AB_HAS_DATA_Flow_2) && (AB_USERCOND_XForm_Error_File)"
   if [ X"${AB_IS_LIVE_XForm_Error_File}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_XForm_Reject_File}"
   let AB_IS_LIVE_XForm_Reject_File="(AB_HAS_DATA_Flow_10) && (AB_USERCOND_XForm_Reject_File)"
   if [ X"${AB_IS_LIVE_XForm_Reject_File}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Send_From_Extract_Value_to_Rollup}"
   let AB_IS_LIVE_Send_From_Extract_Value_to_Rollup="(AB_HAS_DATA_Flow_3) && (AB_USERCOND_Send_From_Extract_Value_to_Rollup)"
   if [ X"${AB_IS_LIVE_Send_From_Extract_Value_to_Rollup}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_3}"
   let AB_HAS_DATA_Flow_3="(AB_IS_LIVE_Send_From_Extract_Value_to_Rollup) && (AB_IS_LIVE_Get_Max_Value_for_Extract_Field)"
   if [ X"${AB_HAS_DATA_Flow_3}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Get_Max_Value_for_Extract_Field}"
   let AB_IS_LIVE_Get_Max_Value_for_Extract_Field="(((AB_HAS_DATA_Flow_5) || (AB_HAS_DATA_Flow_3)) && (AB_HAS_DATA_Flow_6)) && (AB_USERCOND_Get_Max_Value_for_Extract_Field)"
   if [ X"${AB_IS_LIVE_Get_Max_Value_for_Extract_Field}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_6}"
   let AB_HAS_DATA_Flow_6="(AB_IS_LIVE_Get_Max_Value_for_Extract_Field) && (AB_IS_LIVE_Last_Extract_Value_File_1)"
   if [ X"${AB_HAS_DATA_Flow_6}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Last_Extract_Value_File_1}"
   let AB_IS_LIVE_Last_Extract_Value_File_1="AB_USERCOND_Last_Extract_Value_File_1"
   if [ X"${AB_IS_LIVE_Last_Extract_Value_File_1}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Send_Primer_Row_to_Reformat}"
   let AB_IS_LIVE_Send_Primer_Row_to_Reformat="(AB_HAS_DATA_Flow_4) && (AB_USERCOND_Send_Primer_Row_to_Reformat)"
   if [ X"${AB_IS_LIVE_Send_Primer_Row_to_Reformat}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_4}"
   let AB_HAS_DATA_Flow_4="(AB_IS_LIVE_Send_Primer_Row_to_Reformat) && (AB_IS_LIVE_Update_Last_Extract_Value_File)"
   if [ X"${AB_HAS_DATA_Flow_4}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Update_Last_Extract_Value_File}"
   let AB_IS_LIVE_Update_Last_Extract_Value_File="((AB_HAS_DATA_Flow_4) && (((AB_HAS_DATA_Flow_12) != 0))) && (AB_USERCOND_Update_Last_Extract_Value_File)"
   if [ X"${AB_IS_LIVE_Update_Last_Extract_Value_File}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_12}"
   let AB_HAS_DATA_Flow_12="(AB_IS_LIVE_Update_Last_Extract_Value_File) && (AB_IS_LIVE_Last_Extract_Value_File)"
   if [ X"${AB_HAS_DATA_Flow_12}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_Last_Extract_Value_File}"
   let AB_IS_LIVE_Last_Extract_Value_File="AB_USERCOND_Last_Extract_Value_File"
   if [ X"${AB_IS_LIVE_Last_Extract_Value_File}" != X"$Temp" ]; then
      done=false
   fi
done
# 
if [ X"${AB_VERBOSE_CONDITIONS}" != X"" ]; then
   # 
   # echo condition variables
   # 
   print -r -- 'AB_USERCOND_single_hdfs_extract=1'
   print -r -- 'AB_IS_LIVE_single_hdfs_extract=1'
   print -r -- 'AB_USERCOND_Hadoop_Input_Dataset='"${AB_USERCOND_Hadoop_Input_Dataset}"
   print -r -- 'AB_IS_LIVE_Hadoop_Input_Dataset='"${AB_IS_LIVE_Hadoop_Input_Dataset}"
   print -r -- 'AB_HAS_DATA_Flow_24='"${AB_HAS_DATA_Flow_24}"
   print -r -- 'AB_USERCOND_Parallel_Read_Hadoop='"${AB_USERCOND_Parallel_Read_Hadoop}"
   print -r -- 'AB_IS_LIVE_Parallel_Read_Hadoop='"${AB_IS_LIVE_Parallel_Read_Hadoop}"
   print -r -- 'AB_HAS_DATA_Flow_1='"${AB_HAS_DATA_Flow_1}"
   print -r -- 'AB_USERCOND_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_=1'
   print -r -- 'AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_='"${AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_}"
   print -r -- 'AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1='"${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1}"
   print -r -- 'AB_USERCOND_Parallel_Read_Hadoop_Reformat=1'
   print -r -- 'AB_IS_LIVE_Parallel_Read_Hadoop_Reformat='"${AB_IS_LIVE_Parallel_Read_Hadoop_Reformat}"
   print -r -- 'AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2='"${AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2}"
   print -r -- 'AB_USERCOND_Parallel_Read_Hadoop_Redefine_Format=1'
   print -r -- 'AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format='"${AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format}"
   print -r -- 'AB_USERCOND_Replicate='"${AB_USERCOND_Replicate}"
   print -r -- 'AB_IS_LIVE_Replicate='"${AB_IS_LIVE_Replicate}"
   print -r -- 'AB_HAS_DATA_Flow_16='"${AB_HAS_DATA_Flow_16}"
   print -r -- 'AB_HAS_DATA_Flow_5='"${AB_HAS_DATA_Flow_5}"
   print -r -- 'AB_USERCOND_Conditional_Partition_by_Round_robin='"${AB_USERCOND_Conditional_Partition_by_Round_robin}"
   print -r -- 'AB_IS_LIVE_Conditional_Partition_by_Round_robin='"${AB_IS_LIVE_Conditional_Partition_by_Round_robin}"
   print -r -- 'AB_HAS_DATA_Flow_11='"${AB_HAS_DATA_Flow_11}"
   print -r -- 'AB_USERCOND_Retain_Flow_to_Flow='"${AB_USERCOND_Retain_Flow_to_Flow}"
   print -r -- 'AB_IS_LIVE_Retain_Flow_to_Flow='"${AB_IS_LIVE_Retain_Flow_to_Flow}"
   print -r -- 'AB_HAS_DATA_Flow_23='"${AB_HAS_DATA_Flow_23}"
   print -r -- 'AB_HAS_DATA_Flow_7='"${AB_HAS_DATA_Flow_7}"
   print -r -- 'AB_USERCOND_Compute_Checksum_of_Input_Flow=1'
   print -r -- 'AB_IS_LIVE_Compute_Checksum_of_Input_Flow='"${AB_IS_LIVE_Compute_Checksum_of_Input_Flow}"
   print -r -- 'AB_HAS_DATA_Flow_8='"${AB_HAS_DATA_Flow_8}"
   print -r -- 'AB_USERCOND_Capture_out_record_count_from_Checksum=1'
   print -r -- 'AB_IS_LIVE_Capture_out_record_count_from_Checksum='"${AB_IS_LIVE_Capture_out_record_count_from_Checksum}"
   print -r -- 'AB_HAS_DATA_Flow_9='"${AB_HAS_DATA_Flow_9}"
   print -r -- 'AB_USERCOND_Record_Count_File=1'
   print -r -- 'AB_IS_LIVE_Record_Count_File=1'
   print -r -- 'AB_USERCOND_Conditional_Extract_Reformat='"${AB_USERCOND_Conditional_Extract_Reformat}"
   print -r -- 'AB_IS_LIVE_Conditional_Extract_Reformat='"${AB_IS_LIVE_Conditional_Extract_Reformat}"
   print -r -- 'AB_HAS_DATA_Flow_21='"${AB_HAS_DATA_Flow_21}"
   print -r -- 'AB_HAS_DATA_Flow_10='"${AB_HAS_DATA_Flow_10}"
   print -r -- 'AB_HAS_DATA_Flow_2='"${AB_HAS_DATA_Flow_2}"
   print -r -- 'AB_USERCOND_Deflate='"${AB_USERCOND_Deflate}"
   print -r -- 'AB_IS_LIVE_Deflate='"${AB_IS_LIVE_Deflate}"
   print -r -- 'AB_HAS_DATA_Flow_22='"${AB_HAS_DATA_Flow_22}"
   print -r -- 'AB_USERCOND_Output_File=1'
   print -r -- 'AB_IS_LIVE_Output_File=1'
   print -r -- 'AB_USERCOND_XForm_Error_File='"${AB_USERCOND_XForm_Error_File}"
   print -r -- 'AB_IS_LIVE_XForm_Error_File='"${AB_IS_LIVE_XForm_Error_File}"
   print -r -- 'AB_USERCOND_XForm_Reject_File='"${AB_USERCOND_XForm_Reject_File}"
   print -r -- 'AB_IS_LIVE_XForm_Reject_File='"${AB_IS_LIVE_XForm_Reject_File}"
   print -r -- 'AB_USERCOND_Send_From_Extract_Value_to_Rollup='"${AB_USERCOND_Send_From_Extract_Value_to_Rollup}"
   print -r -- 'AB_IS_LIVE_Send_From_Extract_Value_to_Rollup='"${AB_IS_LIVE_Send_From_Extract_Value_to_Rollup}"
   print -r -- 'AB_HAS_DATA_Flow_3='"${AB_HAS_DATA_Flow_3}"
   print -r -- 'AB_USERCOND_Get_Max_Value_for_Extract_Field='"${AB_USERCOND_Get_Max_Value_for_Extract_Field}"
   print -r -- 'AB_IS_LIVE_Get_Max_Value_for_Extract_Field='"${AB_IS_LIVE_Get_Max_Value_for_Extract_Field}"
   print -r -- 'AB_HAS_DATA_Flow_6='"${AB_HAS_DATA_Flow_6}"
   print -r -- 'AB_USERCOND_Last_Extract_Value_File_1='"${AB_USERCOND_Last_Extract_Value_File_1}"
   print -r -- 'AB_IS_LIVE_Last_Extract_Value_File_1='"${AB_IS_LIVE_Last_Extract_Value_File_1}"
   print -r -- 'AB_USERCOND_Send_Primer_Row_to_Reformat='"${AB_USERCOND_Send_Primer_Row_to_Reformat}"
   print -r -- 'AB_IS_LIVE_Send_Primer_Row_to_Reformat='"${AB_IS_LIVE_Send_Primer_Row_to_Reformat}"
   print -r -- 'AB_HAS_DATA_Flow_4='"${AB_HAS_DATA_Flow_4}"
   print -r -- 'AB_USERCOND_Update_Last_Extract_Value_File='"${AB_USERCOND_Update_Last_Extract_Value_File}"
   print -r -- 'AB_IS_LIVE_Update_Last_Extract_Value_File='"${AB_IS_LIVE_Update_Last_Extract_Value_File}"
   print -r -- 'AB_HAS_DATA_Flow_12='"${AB_HAS_DATA_Flow_12}"
   print -r -- 'AB_USERCOND_Last_Extract_Value_File='"${AB_USERCOND_Last_Extract_Value_File}"
   print -r -- 'AB_IS_LIVE_Last_Extract_Value_File='"${AB_IS_LIVE_Last_Extract_Value_File}"
fi

# Files:
mp ofile Record_Count_File 'file:'"$RECORD_COUNT_FILE"
mp ofile Output_File "$OUTPUT_FILE"
if [ X"${AB_IS_LIVE_Last_Extract_Value_File_1}" != X0 ]; then
   mp ofile Last_Extract_Value_File_1 'file:'"$LAST_EXTRACT_VALUE_FILE"
else
   :
fi
if [ X"${AB_IS_LIVE_Last_Extract_Value_File}" != X0 ]; then
   mp ofile Last_Extract_Value_File 'file:'"$LAST_EXTRACT_VALUE_FILE"
else
   :
fi

# Components in phase 0:
if [ X"${AB_IS_LIVE_Hadoop_Input_Dataset}" != X0 ]; then
   mp custom Hadoop_Input_Dataset "$AB_COMPONENTS"'/Datasets/Hadoop/Read_Hadoop_Files.mpc' "$INPUT_FILE" "$DW_DBC"'/'"$JOB_CONFIG" -HADOOP_HOME "$HADOOP_HOME" -layout layout1
   AB_PORT_Hadoop_Input_Dataset_out=Hadoop_Input_Dataset.out
   AB_METADATA_Hadoop_Input_Dataset_out=' -metadata metadata1'
else
   :
fi
if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro_}" != X0 ]; then
   mp hadoop-read Parallel_Read_Hadoop.Hadoop_Parallel_Read__macro_ "$DW_DBC"'/'"$JOB_CONFIG" /~ab_home/lib/java/ab_hadoop.jar -job_input "$INPUT_FILE" -job_class com.abinitio.hadoop.job.ExtractMapper -job_options -text -JAVA_HOME "$JAVA_HOME" -HADOOP_HOME "$HADOOP_HOME" -compressed -poll_delay 0.5 -record_size_limit 1048576 -record_time_limit 60.0 -limit 0 -ramp 0.0 -layout layout1
   AB_PORT_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out=Parallel_Read_Hadoop.Hadoop_Parallel_Read__macro_.out
   AB_METADATA_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out=' -metadata metadata2'
else
   :
fi
if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Reformat}" != X0 ]; then
   mp reformat-transform Parallel_Read_Hadoop.Reformat -limit 0 -ramp 0.0 -layout layout1
   let AB_DO_ADD_PORT="AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2"
   if [ X"${AB_DO_ADD_PORT}" != X0 ]; then
      mp add-port Parallel_Read_Hadoop.Reformat.out.out0 ${_AB_PROXY_DIR:+"$_AB_PROXY_DIR"}'/Reformat-4.xfr'
   fi
   AB_PORT_Parallel_Read_Hadoop_Reformat_out_out0=Parallel_Read_Hadoop.Reformat.out.out0
   AB_METADATA_Parallel_Read_Hadoop_Reformat_out_out0=' -metadata metadata3'
else
   AB_PORT_Parallel_Read_Hadoop_Reformat_out_out0="${AB_PORT_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out}"
   AB_METADATA_Parallel_Read_Hadoop_Reformat_out_out0="${AB_METADATA_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out}"
   :
fi
if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format}" != X0 ]; then
   mp copy Parallel_Read_Hadoop.Redefine_Format -layout layout1
   AB_PORT_Parallel_Read_Hadoop_Redefine_Format_out=Parallel_Read_Hadoop.Redefine_Format.out
   AB_METADATA_Parallel_Read_Hadoop_Redefine_Format_out=' -metadata metadata1'
   AB_PORT_Parallel_Read_Hadoop_out0=Parallel_Read_Hadoop.Redefine_Format.out
   AB_METADATA_Parallel_Read_Hadoop_out0=' -metadata metadata1'
else
   AB_PORT_Parallel_Read_Hadoop_Redefine_Format_out="${AB_PORT_Parallel_Read_Hadoop_Reformat_out_out0}"
   AB_METADATA_Parallel_Read_Hadoop_Redefine_Format_out="${AB_METADATA_Parallel_Read_Hadoop_Reformat_out_out0}"
   if [ X"${AB_IS_LIVE_Parallel_Read_Hadoop}" != X0 ]; then
      AB_PORT_Parallel_Read_Hadoop_out0="${AB_PORT_Parallel_Read_Hadoop_Redefine_Format_out}"
      AB_METADATA_Parallel_Read_Hadoop_out0="${AB_METADATA_Parallel_Read_Hadoop_Redefine_Format_out}"
   fi
   :
fi
if [ X"${AB_IS_LIVE_Replicate}" != X0 ]; then
   mp broadcast Replicate -layout layout1
   AB_PORT_Replicate_out=Replicate.out
   AB_METADATA_Replicate_out=' -metadata metadata1'
else
   let AB_PORT_HAS_DATA="AB_HAS_DATA_Flow_1"
   if [ X"${AB_PORT_HAS_DATA}" != X0 ]; then
      AB_PORT_Replicate_out="${AB_PORT_Parallel_Read_Hadoop_out0}"
      AB_METADATA_Replicate_out="${AB_METADATA_Parallel_Read_Hadoop_out0}"
   fi
   let AB_PORT_HAS_DATA="AB_HAS_DATA_Flow_24"
   if [ X"${AB_PORT_HAS_DATA}" != X0 ]; then
      AB_PORT_Replicate_out="${AB_PORT_Hadoop_Input_Dataset_out}"
      AB_METADATA_Replicate_out="${AB_METADATA_Hadoop_Input_Dataset_out}"
   fi
   :
fi
if [ X"${AB_IS_LIVE_Conditional_Partition_by_Round_robin}" != X0 ]; then
   mp roundrobin-partition Conditional_Partition_by_Round_robin 1 -layout layout1
   AB_PORT_Conditional_Partition_by_Round_robin_out=Conditional_Partition_by_Round_robin.out
   AB_METADATA_Conditional_Partition_by_Round_robin_out=' -metadata metadata1'
else
   AB_PORT_Conditional_Partition_by_Round_robin_out="${AB_PORT_Replicate_out}"
   AB_METADATA_Conditional_Partition_by_Round_robin_out="${AB_METADATA_Replicate_out}"
   :
fi
if [ X"${AB_IS_LIVE_Retain_Flow_to_Flow}" != X0 ]; then
   mp broadcast Retain_Flow_to_Flow -layout Output_File
   AB_PORT_Retain_Flow_to_Flow_out=Retain_Flow_to_Flow.out
   AB_METADATA_Retain_Flow_to_Flow_out=' -metadata metadata1'
else
   AB_PORT_Retain_Flow_to_Flow_out="${AB_PORT_Conditional_Partition_by_Round_robin_out}"
   AB_METADATA_Retain_Flow_to_Flow_out="${AB_METADATA_Conditional_Partition_by_Round_robin_out}"
   :
fi
if [ X"${AB_IS_LIVE_Compute_Checksum_of_Input_Flow}" != X0 ]; then
   mp compute-checksum Compute_Checksum_of_Input_Flow -layout Record_Count_File
   AB_PORT_Compute_Checksum_of_Input_Flow_out=Compute_Checksum_of_Input_Flow.out
   AB_METADATA_Compute_Checksum_of_Input_Flow_out=' -metadata metadata4'
else
   AB_PORT_Compute_Checksum_of_Input_Flow_out="${AB_PORT_Retain_Flow_to_Flow_out}"
   AB_METADATA_Compute_Checksum_of_Input_Flow_out="${AB_METADATA_Retain_Flow_to_Flow_out}"
   :
fi
if [ X"${AB_IS_LIVE_Capture_out_record_count_from_Checksum}" != X0 ]; then
   mp reformat-transform Capture_out_record_count_from_Checksum -limit 0 -ramp 0.0 -layout Record_Count_File
   let AB_DO_ADD_PORT="AB_HAS_DATA_Flow_9"
   if [ X"${AB_DO_ADD_PORT}" != X0 ]; then
      mp add-port Capture_out_record_count_from_Checksum.out.out0
   fi
   AB_PORT_Capture_out_record_count_from_Checksum_out_out0=Capture_out_record_count_from_Checksum.out.out0
   AB_METADATA_Capture_out_record_count_from_Checksum_out_out0=' -metadata metadata5'
else
   AB_PORT_Capture_out_record_count_from_Checksum_out_out0="${AB_PORT_Compute_Checksum_of_Input_Flow_out}"
   AB_METADATA_Capture_out_record_count_from_Checksum_out_out0="${AB_METADATA_Compute_Checksum_of_Input_Flow_out}"
   :
fi
if [ X"${AB_IS_LIVE_Conditional_Extract_Reformat}" != X0 ]; then
   mp reformat-transform Conditional_Extract_Reformat -limit 0 -ramp 0.0 -layout Output_File
   let AB_DO_ADD_PORT="AB_HAS_DATA_Flow_21"
   if [ X"${AB_DO_ADD_PORT}" != X0 ]; then
      mp add-port Conditional_Extract_Reformat.out.out0 ${REFORMAT_TRANS_FILE:+"$REFORMAT_TRANS_FILE"}
   fi
   AB_PORT_Conditional_Extract_Reformat_out_out0=Conditional_Extract_Reformat.out.out0
   AB_METADATA_Conditional_Extract_Reformat_out_out0=' -metadata metadata6'
   AB_PORT_Conditional_Extract_Reformat_reject_out0=Conditional_Extract_Reformat.reject.out0
   AB_METADATA_Conditional_Extract_Reformat_reject_out0=' -metadata metadata1'
   AB_PORT_Conditional_Extract_Reformat_error_out0=Conditional_Extract_Reformat.error.out0
   AB_METADATA_Conditional_Extract_Reformat_error_out0=' -metadata metadata7'
else
   AB_PORT_Conditional_Extract_Reformat_out_out0="${AB_PORT_Retain_Flow_to_Flow_out}"
   AB_METADATA_Conditional_Extract_Reformat_out_out0="${AB_METADATA_Retain_Flow_to_Flow_out}"
   AB_PORT_Conditional_Extract_Reformat_reject_out0="${AB_PORT_Retain_Flow_to_Flow_out}"
   AB_METADATA_Conditional_Extract_Reformat_reject_out0="${AB_METADATA_Retain_Flow_to_Flow_out}"
   AB_PORT_Conditional_Extract_Reformat_error_out0="${AB_PORT_Retain_Flow_to_Flow_out}"
   AB_METADATA_Conditional_Extract_Reformat_error_out0="${AB_METADATA_Retain_Flow_to_Flow_out}"
   :
fi
if [ X"${AB_IS_LIVE_Deflate}" != X0 ]; then
   mp broadcast Deflate -compression "$Deflate__compression" -layout Output_File
   AB_PORT_Deflate_out=Deflate.out
   AB_METADATA_Deflate_out=' -metadata metadata1'
else
   AB_PORT_Deflate_out="${AB_PORT_Conditional_Extract_Reformat_out_out0}"
   AB_METADATA_Deflate_out="${AB_METADATA_Conditional_Extract_Reformat_out_out0}"
   :
fi
if [ X"${AB_IS_LIVE_XForm_Error_File}" != X0 ]; then
   mp logger XForm_Error_File "$XFORM_ERROR_FILE" Start End -layout layout2
else
   :
fi
if [ X"${AB_IS_LIVE_XForm_Reject_File}" != X0 ]; then
   mp logger XForm_Reject_File "$XFORM_REJECT_FILE" Start End -layout layout2
else
   :
fi
if [ X"${AB_IS_LIVE_Send_From_Extract_Value_to_Rollup}" != X0 ]; then
   mp generate Send_From_Extract_Value_to_Rollup 1 -expression $ROLLUP_FIELD '$FROM_EXTRACT_VALUE' -layout Last_Extract_Value_File_1
   AB_PORT_Send_From_Extract_Value_to_Rollup_out=Send_From_Extract_Value_to_Rollup.out
   AB_METADATA_Send_From_Extract_Value_to_Rollup_out=' -metadata metadata1'
else
   :
fi
if [ X"${AB_IS_LIVE_Get_Max_Value_for_Extract_Field}" != X0 ]; then
   mp hash-rollup Get_Max_Value_for_Extract_Field '{}' "${_AB_PROXY_DIR}"'/Get_Max_Value_for_Extract_Field-11.xfr' -max-core 67108864 -limit 0 -ramp 0.0 -layout Last_Extract_Value_File_1
   AB_PORT_Get_Max_Value_for_Extract_Field_out=Get_Max_Value_for_Extract_Field.out
   AB_METADATA_Get_Max_Value_for_Extract_Field_out=' -metadata metadata8'
else
   :
fi
mp checkpoint 0

# Components in phase 1:
if [ X"${AB_IS_LIVE_Send_Primer_Row_to_Reformat}" != X0 ]; then
   mp generate Send_Primer_Row_to_Reformat 1 -layout layout3
   AB_PORT_Send_Primer_Row_to_Reformat_out=Send_Primer_Row_to_Reformat.out
   AB_METADATA_Send_Primer_Row_to_Reformat_out=' -metadata metadata9'
else
   :
fi
if [ X"${AB_IS_LIVE_Update_Last_Extract_Value_File}" != X0 ]; then
   mp reformat-transform Update_Last_Extract_Value_File -limit 0 -ramp 0.0 -layout layout3
   let AB_DO_ADD_PORT="AB_HAS_DATA_Flow_12"
   if [ X"${AB_DO_ADD_PORT}" != X0 ]; then
      mp add-port Update_Last_Extract_Value_File.out.out0 ${_AB_PROXY_DIR:+"$_AB_PROXY_DIR"}'/Update_Last_Extract_Value_File-14.xfr'
   fi
   AB_PORT_Update_Last_Extract_Value_File_out_out0=Update_Last_Extract_Value_File.out.out0
   AB_METADATA_Update_Last_Extract_Value_File_out_out0=' -metadata metadata8'
else
   :
fi

# Flows for Entire Graph:
let AB_FLOW_CONDITION="(AB_IS_LIVE_Parallel_Read_Hadoop_Reformat) && (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_1)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Parallel_Read_Hadoop.Flow_1 "${AB_PORT_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out}" Parallel_Read_Hadoop.Reformat.in${AB_METADATA_Parallel_Read_Hadoop_Hadoop_Parallel_Read__macro__out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Parallel_Read_Hadoop_Redefine_Format) && (AB_HAS_DATA_Parallel_Read_Hadoop_Flow_2)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Parallel_Read_Hadoop.Flow_2 "${AB_PORT_Parallel_Read_Hadoop_Reformat_out_out0}" Parallel_Read_Hadoop.Redefine_Format.in${AB_METADATA_Parallel_Read_Hadoop_Reformat_out_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Replicate) && (AB_HAS_DATA_Flow_1)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_1 "${AB_PORT_Parallel_Read_Hadoop_out0}" Replicate.in${AB_METADATA_Parallel_Read_Hadoop_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Replicate) && (AB_HAS_DATA_Flow_24)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_24 "${AB_PORT_Hadoop_Input_Dataset_out}" Replicate.in${AB_METADATA_Hadoop_Input_Dataset_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Conditional_Partition_by_Round_robin) && (AB_HAS_DATA_Flow_16)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_16 "${AB_PORT_Replicate_out}" Conditional_Partition_by_Round_robin.in${AB_METADATA_Replicate_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Retain_Flow_to_Flow) && (AB_HAS_DATA_Flow_11)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp all-to-all-flow Flow_11 "${AB_PORT_Conditional_Partition_by_Round_robin_out}" Retain_Flow_to_Flow.in${AB_METADATA_Conditional_Partition_by_Round_robin_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Compute_Checksum_of_Input_Flow) && (AB_HAS_DATA_Flow_7)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_7 "${AB_PORT_Retain_Flow_to_Flow_out}" Compute_Checksum_of_Input_Flow.in${AB_METADATA_Retain_Flow_to_Flow_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Conditional_Extract_Reformat) && (AB_HAS_DATA_Flow_23)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_23 "${AB_PORT_Retain_Flow_to_Flow_out}" Conditional_Extract_Reformat.in${AB_METADATA_Retain_Flow_to_Flow_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_XForm_Error_File) && (AB_HAS_DATA_Flow_2)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_2 "${AB_PORT_Conditional_Extract_Reformat_error_out0}" XForm_Error_File.in${AB_METADATA_Conditional_Extract_Reformat_error_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_XForm_Reject_File) && (AB_HAS_DATA_Flow_10)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_10 "${AB_PORT_Conditional_Extract_Reformat_reject_out0}" XForm_Reject_File.in${AB_METADATA_Conditional_Extract_Reformat_reject_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Deflate) && (AB_HAS_DATA_Flow_21)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_21 "${AB_PORT_Conditional_Extract_Reformat_out_out0}" Deflate.in${AB_METADATA_Conditional_Extract_Reformat_out_out0}
fi
let AB_FLOW_CONDITION="AB_HAS_DATA_Flow_22"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_22 "${AB_PORT_Deflate_out}" Output_File.write${AB_METADATA_Deflate_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Capture_out_record_count_from_Checksum) && (AB_HAS_DATA_Flow_8)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_8 "${AB_PORT_Compute_Checksum_of_Input_Flow_out}" Capture_out_record_count_from_Checksum.in${AB_METADATA_Compute_Checksum_of_Input_Flow_out}
fi
let AB_FLOW_CONDITION="AB_HAS_DATA_Flow_9"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_9 "${AB_PORT_Capture_out_record_count_from_Checksum_out_out0}" Record_Count_File.write${AB_METADATA_Capture_out_record_count_from_Checksum_out_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Get_Max_Value_for_Extract_Field) && (AB_HAS_DATA_Flow_5)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_5 "${AB_PORT_Replicate_out}" Get_Max_Value_for_Extract_Field.in${AB_METADATA_Replicate_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Get_Max_Value_for_Extract_Field) && (AB_HAS_DATA_Flow_3)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_3 "${AB_PORT_Send_From_Extract_Value_to_Rollup_out}" Get_Max_Value_for_Extract_Field.in${AB_METADATA_Send_From_Extract_Value_to_Rollup_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Last_Extract_Value_File_1) && (AB_HAS_DATA_Flow_6)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_6 "${AB_PORT_Get_Max_Value_for_Extract_Field_out}" Last_Extract_Value_File_1.write${AB_METADATA_Get_Max_Value_for_Extract_Field_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Update_Last_Extract_Value_File) && (AB_HAS_DATA_Flow_4)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_4 "${AB_PORT_Send_Primer_Row_to_Reformat_out}" Update_Last_Extract_Value_File.in${AB_METADATA_Send_Primer_Row_to_Reformat_out}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_Last_Extract_Value_File) && (AB_HAS_DATA_Flow_12)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_12 "${AB_PORT_Update_Last_Extract_Value_File_out_out0}" Last_Extract_Value_File.write${AB_METADATA_Update_Last_Extract_Value_File_out_out0}
fi

if [ X"${AB_VERBOSE_CONDITIONS}" != X"" ]; then
   print -r -- 'Generated graph:'
   mp show
fi
unset AB_COMM_WAIT
export AB_TRACKING_GRAPH_THUMBPRINT;AB_TRACKING_GRAPH_THUMBPRINT=1039820
mp run
mpjret=$?
unset AB_COMM_WAIT
unset AB_TRACKING_GRAPH_THUMBPRINT
mp reset
m_rmcatalog > /dev/null 2>&1
export XX_CATALOG;XX_CATALOG="${SAVED_CATALOG}"
export AB_CATALOG;AB_CATALOG="${SAVED_CATALOG}"

#+Script End+  ==================== Edits in this section are preserved.
#+End Script End+  ====================

exit $mpjret
