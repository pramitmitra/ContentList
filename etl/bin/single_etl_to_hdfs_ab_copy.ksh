#! /bin/ksh
# Script generated by software licensed from Ab Initio.
# Use and disclosure are subject to Ab Initio confidentiality and license terms.
export AB_HOME;AB_HOME=${AB_HOME:-/usr/local/abinitio-V3-1-4}
export MPOWERHOME;MPOWERHOME="$AB_HOME"
export AB_COMPONENTS;AB_COMPONENTS="$AB_HOME"'/Projects/root/components'
export PATH
typeset _ab_uname=`uname`
case "$_ab_uname" in
Windows_* )
    PATH="$AB_HOME/bin;$PATH" ;;
CYGWIN_* )
    PATH="`cygpath "$AB_HOME"`/bin:/usr/local/bin:/usr/bin:/bin:$PATH" ;;
* )
    PATH="$AB_HOME/bin:$PATH" ;;
esac
unset ENV
export AB_REPORT;AB_REPORT=${AB_REPORT:-'monitor=300 processes scroll=true'}
unset GDE_EXECUTION

export AB_COMPATIBILITY;AB_COMPATIBILITY=3.1.4.4

# Deployed execution script for graph "single_etl_to_hdfs_ab_copy", compiled at Thursday, June 28, 2018 11:43:51 using GDE version 3.1.4.1
export AB_JOB;AB_JOB=${AB_JOB_PREFIX:-""}single_etl_to_hdfs_ab_copy
# Begin Ab Initio shell utility functions

: ${_ab_uname:=$(uname)}

function __AB_INVOKE_PROJECT
{
  typeset _AB_PROJECT_KSH="$1" ; shift
  typeset _AB_PROJECT_DIR="$1" ; shift
  typeset _AB_DEFINE_OR_EXECUTE="$1" ; shift
  typeset _AB_START_OR_END="$1" ; shift
  # Check that the project exists:
  if [ ! -r "$_AB_PROJECT_KSH" ] ; then
    print -r -u2 Warning: Cannot find common sandbox script: "$_AB_PROJECT_KSH"
    if [ ! -z "${_AB_CALLING_PROJECT:=}" ] ; then
      print -r -u2 Please check the common sandbox settings for the calling project: "$_AB_CALLING_PROJECT"
    fi
  fi
  if [ $# -gt 0 ] ; then
    . "$_AB_PROJECT_KSH" "$_AB_PROJECT_DIR" "$_AB_DEFINE_OR_EXECUTE" "$_AB_START_OR_END"  "$@"
  else
    . "$_AB_PROJECT_KSH" "$_AB_PROJECT_DIR" "$_AB_DEFINE_OR_EXECUTE" "$_AB_START_OR_END" 
  fi;
}

function __AB_DOTIT
{
  if [ $# -gt 0 ] ; then
    .  "$@"
  fi
}

function __AB_QUOTEIT {
  typeset queue q qq qed lotsaqs s trail
  q="'"
  qq='"'
  if [ X"$1" = X"" ] ; then
    print $q$q
    return
  fi
  lotsaqs=${q}${qq}${q}${qq}${q}
  if [ ${#1} -ge 10000 ]; then
    print -r -- "$1" | sed "s/$q/$lotsaqs/g; 1s/^/$q/; \$s/\$/$q/"
  else
    queue=${1%$q}
    if [ X"$queue" != X"$1" ] ; then
      trail="${qq}${q}${qq}" 
    else 
      trail=""
    fi
    oldIFS="$IFS"
    IFS=$q
    set -- $queue
    IFS="$oldIFS"
    print -rn "$q$1"
    shift
    for s; do
      print -rn "$lotsaqs$s"
    done
    print -r $q$trail
  fi
}

function __AB_dirname {
    case $_ab_uname in
    Windows_* | CYGWIN_* )
        typeset d='' p="$1"
        # Strip drive letter colon, if present, and put it into d.
        case $p in
        [A-Za-z]:* )
            d=${p%%:*}:
            p=${p#??}
            ;;
        esac
        # Remove trailing separators, though not the last character in the
        # pathname.
        while : true; do
            case $p in
            ?*[/\\] )
                p=${p%[/\\]} ;;
            * )
                break ;;
            esac
        done
        if [[ "$p" = ?*[/\\]* ]] ; then
            print -r -- "$d${p%[/\\]*}"
        elif [[ "$p" = [/\\]* ]] ; then
            print "$d/"
        else
            print "$d." 
        fi
        ;;
    * ) # Unix
        typeset p="$1"
        # Remove trailing separators, though not the last character in the
        # pathname.
        while : true; do
            case $p in
            ?*/ )
                p="${p%/}" ;;
            * )
                break ;;
            esac
        done
        case $p in
        ?*/* )
            print -r -- "${p%/*}" ;;
        /* )
            print / ;;
        * )
            print . ;;
        esac
        ;;
    esac
}

function __AB_concat_pathname {
    case $_ab_uname in
    Windows_* | CYGWIN_* )
        # Does not handle all cases of concatenating partially absolute
        # pathnames, those with only one of a drive letter or an initial
        # separator.
        case $2 in
        [/\\]* | [A-Za-z]:* )
            print -r -- "$2"
            ;;
        * )
            case $1 in
            # Assume that empty string means ".".  Avoid adding a
            # redundant separator.
            '' | *[/\\] )
                print -r -- "$1$2" ;;
            * )
                print -r -- "$1/$2" ;;
            esac
            ;;
        esac
        ;;
    * ) # Unix
        case $2 in
        /* )
            print -r -- "$2"
            ;;
        * )
            case $1 in
            # Assume that empty string means ".".  Avoid adding a
            # redundant separator.
            '' | */ )
                print -r -- "$1$2" ;;
            * )
                print -r -- "$1/$2" ;;
            esac
            ;;
        esac
        ;;
    esac
}

function __AB_COND {
if [ X"$1" = X0  -o X"$1" = Xfalse -o X"$1" = XFalse -o X"$1" = XF -o X"$1" = Xf ] ; then
  print "0"
else
  print "1"
fi
}

# End Ab Initio shell utility functions
export AB_GRAPH_NAME;AB_GRAPH_NAME=single_etl_to_hdfs_ab_copy

_AB_PROXY_DIR="$(pwd)"/single_etl_to_hdfs_ab_copy-ProxyDir-$$
rm -rf "${_AB_PROXY_DIR}"
mkdir "${_AB_PROXY_DIR}"
print -r -- "" > "${_AB_PROXY_DIR}"'/GDE-Parameters'
function __AB_CLEANUP_PROXY_FILES
{
   rm -rf "${_AB_PROXY_DIR}"
   rm -rf "${AB_EXTERNAL_PROXY_DIR}"
   return
}
trap '__AB_CLEANUP_PROXY_FILES' EXIT
# Work around pdksh bug: the EXIT handler is not executed upon a signal.
trap '_AB_status=$?; __AB_CLEANUP_PROXY_FILES; exit $_AB_status' HUP INT QUIT TERM
if [ $# -gt 0 -a X"$1" = X"-help" ]; then
print -r -- 'Usage: single_etl_to_hdfs_ab_copy.ksh <ETL_ID> <JOB_ENV> <INPUT_FILE> <OUTPUT_FILE>'
exit 1
fi

# Command Line Processing
function _AB_PARSE_ARGUMENTS {
   unset ETL_ID
   unset JOB_ENV
   unset INPUT_FILE
   unset OUTPUT_FILE
   _ab_index_var=0
   if [ $# -gt 0 ]; then
      export ETL_ID;      ETL_ID="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export JOB_ENV;      JOB_ENV="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export INPUT_FILE;      INPUT_FILE="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   if [ $# -gt 0 ]; then
      export OUTPUT_FILE;      OUTPUT_FILE="${1}"
      let _ab_index_var=_ab_index_var+1
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      shift
   fi
   while [ $# -gt 0 ]; do
   _ab_kwd="${1}"
   let _ab_index_var=_ab_index_var+1
   shift
   case ${_ab_kwd} in
     -USE_OVRD_DML )
      USE_OVRD_DML="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -CNDTL_COMPRESSION )
      CNDTL_COMPRESSION="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -CNDTL_COMPRESSION_SFX )
      CNDTL_COMPRESSION_SFX="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
     -HADOOP_CONN )
      HADOOP_CONN="${1}"
      _AB_USED_ARGUMENTS[_ab_index_var]=1
      _AB_USED_ARGUMENTS[_ab_index_var+1]=1
      let _ab_index_var=_ab_index_var+1
      shift
      ;;
   * )
      if [ X"${_AB_USED_ARGUMENTS[_ab_index_var]}" != X1 ]; then
         print -r -- 'Unexpected command line argument found: '"${_ab_kwd}"
         print -r -- 'Usage: single_etl_to_hdfs_ab_copy.ksh <ETL_ID> <JOB_ENV> <INPUT_FILE> <OUTPUT_FILE>'
         exit 1
      fi
   esac
   done
}
if [ $# -gt 0 ]; then
   _AB_PARSE_ARGUMENTS "$@"
else
   _AB_PARSE_ARGUMENTS
fi

if [ X"${ETL_ID:-}" = X"" ]; then
   print -r -- 'Required parameter ETL_ID undefined'
   print -r -- 'Usage: single_etl_to_hdfs_ab_copy.ksh <ETL_ID> <JOB_ENV> <INPUT_FILE> <OUTPUT_FILE>'
   exit 1
fi

if [ X"${JOB_ENV:-}" = X"" ]; then
   print -r -- 'Required parameter JOB_ENV undefined'
   print -r -- 'Usage: single_etl_to_hdfs_ab_copy.ksh <ETL_ID> <JOB_ENV> <INPUT_FILE> <OUTPUT_FILE>'
   exit 1
fi
INPUT_FILE=${INPUT_FILE:-""}
OUTPUT_FILE=${OUTPUT_FILE:-""}
export OVRD_DML_FILENAME;OVRD_DML_FILENAME="$ETL_ID"'.read.ovrd.dml'
export JOB_TYPE;JOB_TYPE=hdfs_load
export DW_CFG;DW_CFG="$DW_CFG"
export DW_DML;DW_DML="$DW_DML"
export DW_EXE;DW_EXE="$DW_EXE"
export ETL_CFG_FILE;ETL_CFG_FILE="$DW_CFG"'/'"$ETL_ID"'.cfg'
export SUBJECT_AREA;SUBJECT_AREA=${ETL_ID%%.*}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter SUBJECT_AREA of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export TABLE_ID;TABLE_ID=${ETL_ID##*.}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter TABLE_ID of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export AB_JOB;AB_JOB=$(if [ $ETL_ENV ]
then
   print $AB_JOB.$ETL_ID.$ETL_ENV.$JOB_ENV
else
   print $AB_JOB.$ETL_ID.$JOB_ENV
fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter AB_JOB of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export DW_SA_ARC;DW_SA_ARC="$DW_ARC"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export DW_SA_DAT;DW_SA_DAT="$DW_DAT"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export DW_SA_IN;DW_SA_IN="$DW_IN"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export DW_SA_LOG;DW_SA_LOG=${DW_SA_LOG:-$DW_LOG/$JOB_ENV/$SUBJECT_AREA}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter DW_SA_LOG of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export DW_SA_TMP;DW_SA_TMP="$DW_TMP"'/'"$JOB_ENV"'/'"$SUBJECT_AREA"
export FILE_DATETIME;FILE_DATETIME=${CURR_DATETIME:-$(date '+%Y%m%d-%H%M%S')}
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter FILE_DATETIME of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
USE_OVRD_DML=$(grep "^USE_OVRD_DML\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter USE_OVRD_DML of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export INPUT_DML;INPUT_DML=$(if [[ $USE_OVRD_DML -eq 1 ]]
  then
      print $DW_DML/$OVRD_DML_FILENAME
  else
      print $DW_DML/$ETL_ID.read.dml
  fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter INPUT_DML of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
CNDTL_COMPRESSION=$(grep "^CNDTL_COMPRESSION\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_COMPRESSION of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
CNDTL_COMPRESSION_SFX=$(grep "^CNDTL_COMPRESSION_SFX\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print $VALUE)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_COMPRESSION_SFX of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export CNDTL_REFORMAT;CNDTL_REFORMAT=$(grep "^CNDTL_REFORMAT\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter CNDTL_REFORMAT of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export REFORMAT_TRANS_FILE;REFORMAT_TRANS_FILE="$DW_XFR"'/'"$ETL_ID"'.reformat.xfr'
export OUTPUT_DML;OUTPUT_DML=$(if (($CNDTL_REFORMAT))
then
   print $DW_DML/$ETL_ID.write.dml
else
   print $INPUT_DML
fi)
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter OUTPUT_DML of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
XFORM_REJLIMIT=$(grep "^HDFS_LOAD_XFORM_REJLIMIT\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter XFORM_REJLIMIT of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
XFORM_REJRAMP=$(grep "^HDFS_LOAD_XFORM_REJRAMP\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; print ${VALUE:-0.0})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter XFORM_REJRAMP of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
export XFORM_REJFILE;XFORM_REJFILE="$DW_SA_TMP"'/'"$TABLE_ID"'.ld.reformat.rej'
export XFORM_ERRFILE;XFORM_ERRFILE="$DW_SA_TMP"'/'"$TABLE_ID"'.ld.reformat.err'
HADOOP_CONN=$(grep "^HDFS_TGT_CONN\>" $ETL_CFG_FILE | read PARAM VALUE COMMENT; eval print ${VALUE:-$DW_DBC/hdfs_$HD_CLUSTER.xml})
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'parameter HADOOP_CONN of single_etl_to_hdfs_ab_copy', interpretation 'shell'
   exit $mpjret
fi
. "${_AB_PROXY_DIR}"'/GDE-Parameters'

#+Script Start+  ==================== 
m_env -v

if [ -z $CNDTL_REFORMAT ]
then
   print "$0: Error: CNDTL_REFORMAT variable not set"
   exit 4
fi

export AB_REPORT="$AB_REPORT error-codes"

#+End Script Start+  ====================
# Check that the "mp" program is found correctly on the PATH
case "$_ab_uname" in
  Windows_* )
    _ab_expected_mp=$AB_HOME/bin/mp.exe ;;
  * )
    _ab_expected_mp=$AB_HOME/bin/mp
esac
if [ ! -x "$_ab_expected_mp" ]; then
  print "\n*** ERROR: executable $_ab_expected_mp not found"
  exit 1
fi
_ab_found_mp=$(whence mp)
if [ "$_ab_found_mp" = "" ] || [ "$_ab_found_mp" -ot "$_ab_expected_mp" ] || [ "$_ab_found_mp" -nt "$_ab_expected_mp" ]; then
  if [ "$_ab_found_mp" = "" ]; then
    print "\n*** ERROR: mp not found on PATH"
  else
    case "$_ab_uname" in
      CYGWIN_* )
        _ab_found_mp=`cygpath -m "$_ab_found_mp"` ;;
    esac
    print "\n*** ERROR: Wrong mp found on the PATH: $_ab_found_mp"
    print "           Should be via \$AB_HOME/bin: $_ab_expected_mp"
  fi
  print "\nCheck Setup Script in Host Connections Settings and Script Start in Graph Settings for PATH modifications"
  print "Active PATH=$PATH"
  exit 1
fi
if [ -f "$AB_HOME/bin/ab_catalog_functions.ksh" ]; then . ab_catalog_functions.ksh; fi
mv "${_AB_PROXY_DIR}" "$(pwd)"/"${AB_JOB}"'-single_etl_to_hdfs_ab_copy-ProxyDir'
_AB_PROXY_DIR="$(pwd)"/"${AB_JOB}"'-single_etl_to_hdfs_ab_copy-ProxyDir'
print -r -- 'string('"'"'\n'"'"')' > "${_AB_PROXY_DIR}"'/Load_Reformat-4.dml'

mp job ${AB_JOB}

# Layouts:
mp layout layout1 file:.

# Record Formats (Metadata):
mp metadata metadata1 -file "$INPUT_DML"
mp metadata metadata2 -file "$OUTPUT_DML"
mp metadata metadata3 -file "${_AB_PROXY_DIR}"'/Load_Reformat-4.dml'

export AB_CATALOG;AB_CATALOG=${AB_CATALOG:-"${XX_CATALOG}"}
# Catalog Usage: Creating temporary catalog using lookup files only
m_rmcatalog -catalog GDE-single_etl_to_hdfs_ab_copy-${AB_JOB}.cat > /dev/null 2>&1
m_mkcatalog -catalog GDE-single_etl_to_hdfs_ab_copy-${AB_JOB}.cat
SAVED_CATALOG="${AB_CATALOG}"
export AB_CATALOG;AB_CATALOG='GDE-single_etl_to_hdfs_ab_copy-'"${AB_JOB}"'.cat'
# 
# Initialize condition variables to user-specified conditions
# 
AB_USERCOND_single_etl_to_hdfs_ab_copy=1
AB_IS_LIVE_single_etl_to_hdfs_ab_copy=1
AB_USERCOND_Input_File=1
mpjret=$?
if [ 0 -ne $mpjret ] ; then
   print -- Error evaluating: 'AB_USERCOND_Input_File', interpretation 'shell'
   exit $mpjret
fi
AB_IS_LIVE_Input_File=1
AB_HAS_DATA_Flow_2=1
AB_USERCOND_Load_Reformat="$CNDTL_REFORMAT"
AB_USERCOND_Load_Reformat=$(__AB_COND "${AB_USERCOND_Load_Reformat}")
AB_IS_LIVE_Load_Reformat=1
AB_HAS_DATA_Flow_1=1
AB_HAS_DATA_Flow_13=1
AB_HAS_DATA_Flow_33=1
AB_USERCOND_Write_Hadoop_Files=1
AB_IS_LIVE_Write_Hadoop_Files=1
AB_USERCOND_XForm_Error_File="$CNDTL_REFORMAT"
AB_USERCOND_XForm_Error_File=$(__AB_COND "${AB_USERCOND_XForm_Error_File}")
AB_IS_LIVE_XForm_Error_File=1
AB_USERCOND_XForm_Reject_File="$CNDTL_REFORMAT"
AB_USERCOND_XForm_Reject_File=$(__AB_COND "${AB_USERCOND_XForm_Reject_File}")
AB_IS_LIVE_XForm_Reject_File=1
# 
# Compute condition variables by considering the conditions of neighboring components
# 
done=false
while [ $done = false ] ; do
   done=true
   Temp="${AB_IS_LIVE_Load_Reformat}"
   let AB_IS_LIVE_Load_Reformat="AB_USERCOND_Load_Reformat"
   if [ X"${AB_IS_LIVE_Load_Reformat}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_13}"
   let AB_HAS_DATA_Flow_13="(AB_IS_LIVE_Load_Reformat) && (AB_IS_LIVE_XForm_Reject_File)"
   if [ X"${AB_HAS_DATA_Flow_13}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_HAS_DATA_Flow_33}"
   let AB_HAS_DATA_Flow_33="(AB_IS_LIVE_Load_Reformat) && (AB_IS_LIVE_XForm_Error_File)"
   if [ X"${AB_HAS_DATA_Flow_33}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_XForm_Error_File}"
   let AB_IS_LIVE_XForm_Error_File="(AB_HAS_DATA_Flow_33) && (AB_USERCOND_XForm_Error_File)"
   if [ X"${AB_IS_LIVE_XForm_Error_File}" != X"$Temp" ]; then
      done=false
   fi
   Temp="${AB_IS_LIVE_XForm_Reject_File}"
   let AB_IS_LIVE_XForm_Reject_File="(AB_HAS_DATA_Flow_13) && (AB_USERCOND_XForm_Reject_File)"
   if [ X"${AB_IS_LIVE_XForm_Reject_File}" != X"$Temp" ]; then
      done=false
   fi
done
# 
if [ X"${AB_VERBOSE_CONDITIONS}" != X"" ]; then
   # 
   # echo condition variables
   # 
   print -r -- 'AB_USERCOND_single_etl_to_hdfs_ab_copy=1'
   print -r -- 'AB_IS_LIVE_single_etl_to_hdfs_ab_copy=1'
   print -r -- 'AB_USERCOND_Input_File=1'
   print -r -- 'AB_IS_LIVE_Input_File=1'
   print -r -- 'AB_HAS_DATA_Flow_2=1'
   print -r -- 'AB_USERCOND_Load_Reformat='"${AB_USERCOND_Load_Reformat}"
   print -r -- 'AB_IS_LIVE_Load_Reformat='"${AB_IS_LIVE_Load_Reformat}"
   print -r -- 'AB_HAS_DATA_Flow_1=1'
   print -r -- 'AB_HAS_DATA_Flow_13='"${AB_HAS_DATA_Flow_13}"
   print -r -- 'AB_HAS_DATA_Flow_33='"${AB_HAS_DATA_Flow_33}"
   print -r -- 'AB_USERCOND_Write_Hadoop_Files=1'
   print -r -- 'AB_IS_LIVE_Write_Hadoop_Files=1'
   print -r -- 'AB_USERCOND_XForm_Error_File='"${AB_USERCOND_XForm_Error_File}"
   print -r -- 'AB_IS_LIVE_XForm_Error_File='"${AB_IS_LIVE_XForm_Error_File}"
   print -r -- 'AB_USERCOND_XForm_Reject_File='"${AB_USERCOND_XForm_Reject_File}"
   print -r -- 'AB_IS_LIVE_XForm_Reject_File='"${AB_IS_LIVE_XForm_Reject_File}"
fi

# Files:
mp ifile Input_File $(cat $INPUT_FILE)
AB_PORT_Input_File_read=Input_File.read
AB_METADATA_Input_File_read=' -metadata metadata1'

# Components in phase 0:
if [ X"${AB_IS_LIVE_Load_Reformat}" != X0 ]; then
   mp reformat-transform Load_Reformat -limit "$XFORM_REJLIMIT" -ramp "$XFORM_REJRAMP" -layout Input_File
   mp add-port Load_Reformat.out.out0 ${REFORMAT_TRANS_FILE:+"$REFORMAT_TRANS_FILE"}
   AB_PORT_Load_Reformat_out_out0=Load_Reformat.out.out0
   AB_METADATA_Load_Reformat_out_out0=' -metadata metadata2'
   AB_PORT_Load_Reformat_reject_out0=Load_Reformat.reject.out0
   AB_METADATA_Load_Reformat_reject_out0=' -metadata metadata1'
   AB_PORT_Load_Reformat_error_out0=Load_Reformat.error.out0
   AB_METADATA_Load_Reformat_error_out0=' -metadata metadata3'
else
   AB_PORT_Load_Reformat_out_out0="${AB_PORT_Input_File_read}"
   AB_METADATA_Load_Reformat_out_out0="${AB_METADATA_Input_File_read}"
   AB_PORT_Load_Reformat_reject_out0="${AB_PORT_Input_File_read}"
   AB_METADATA_Load_Reformat_reject_out0="${AB_METADATA_Input_File_read}"
   AB_PORT_Load_Reformat_error_out0="${AB_PORT_Input_File_read}"
   AB_METADATA_Load_Reformat_error_out0="${AB_METADATA_Input_File_read}"
   :
fi
mp custom Write_Hadoop_Files "$AB_COMPONENTS"'/Datasets/Hadoop/Write_Hadoop_Files.mpc' "$OUTPUT_FILE" "$HADOOP_CONN" -compression none -permissions 644 -HADOOP_HOME "$HADOOP_HOME" -layout Input_File
if [ X"${AB_IS_LIVE_XForm_Error_File}" != X0 ]; then
   mp logger XForm_Error_File "$XFORM_ERRFILE" Start End -layout layout1
else
   :
fi
if [ X"${AB_IS_LIVE_XForm_Reject_File}" != X0 ]; then
   mp logger XForm_Reject_File "$XFORM_REJFILE" Start End -layout layout1
else
   :
fi

# Flows for Entire Graph:
let AB_FLOW_CONDITION="AB_IS_LIVE_Load_Reformat"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp straight-flow Flow_2 "${AB_PORT_Input_File_read}" Load_Reformat.in${AB_METADATA_Input_File_read}
fi
mp straight-flow Flow_1 "${AB_PORT_Load_Reformat_out_out0}" Write_Hadoop_Files.in${AB_METADATA_Load_Reformat_out_out0}
let AB_FLOW_CONDITION="(AB_IS_LIVE_XForm_Error_File) && (AB_HAS_DATA_Flow_33)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_33 "${AB_PORT_Load_Reformat_error_out0}" XForm_Error_File.in${AB_METADATA_Load_Reformat_error_out0}
fi
let AB_FLOW_CONDITION="(AB_IS_LIVE_XForm_Reject_File) && (AB_HAS_DATA_Flow_13)"
if [ X"${AB_FLOW_CONDITION}" != X0 ]; then
   mp fan-in-flow Flow_13 "${AB_PORT_Load_Reformat_reject_out0}" XForm_Reject_File.in${AB_METADATA_Load_Reformat_reject_out0}
fi

if [ X"${AB_VERBOSE_CONDITIONS}" != X"" ]; then
   print -r -- 'Generated graph:'
   mp show
fi
unset AB_COMM_WAIT
export AB_TRACKING_GRAPH_THUMBPRINT;AB_TRACKING_GRAPH_THUMBPRINT=1084086
mp run
mpjret=$?
unset AB_COMM_WAIT
unset AB_TRACKING_GRAPH_THUMBPRINT
mp reset
m_rmcatalog > /dev/null 2>&1
export XX_CATALOG;XX_CATALOG="${SAVED_CATALOG}"
export AB_CATALOG;AB_CATALOG="${SAVED_CATALOG}"

#+Script End+  ==================== 




























#+End Script End+  ====================

exit $mpjret
